# 开源 Omni 多模态大模型深度技术分析与对比

> 全面解析 LongCat-Flash-Omni、Qwen3-Omni 等主流开源全模态大模型的架构设计与技术细节

## 📋 目录

- [项目简介](#项目简介)
- [开源 Omni 模型全景对比](#开源-omni-模型全景对比)
- [LongCat-Flash-Omni 深度解析](#longcat-flash-omni-深度解析)
- [Qwen3-Omni 深度解析](#qwen3-omni-深度解析)
- [核心架构对比分析](#核心架构对比分析)
- [技术演进趋势](#技术演进趋势)
- [选型建议](#选型建议)

---

## 项目简介

本项目旨在深入分析当前主流的开源 Omni（全模态）多模态大模型，重点对比：

| 模型 | 机构 | 发布时间 | 核心特点 |
|------|------|----------|----------|
| **LongCat-Flash-Omni** | 美团 | 2024 | 560B MoE，128K长上下文 |
| **Qwen3-Omni** | 阿里巴巴 | 2025.09 | Thinker-Talker双MoE，234ms超低延迟 |
| **MiniCPM-o 2.6** | 面壁智能 | 2025.01 | 8B端侧可用，全双工流式 |
| **GLM-4-Voice** | 智谱AI | 2024.10 | 端到端情感语音 |
| **Janus-Pro** | DeepSeek | 2025.01 | 理解生成统一架构 |

---

## 开源 Omni 模型全景对比

### 核心参数对比表

| 维度 | LongCat-Flash-Omni | Qwen3-Omni | MiniCPM-o 2.6 | GLM-4-Voice | Janus-Pro |
|------|-------------------|------------|---------------|-------------|-----------|
| **总参数** | 560B | 34B (Thinker+Talker) | 8B | - | 7B |
| **激活参数** | 27B (4.8%) | Thinker 3B / Talker 0.3B | 8B | - | 7B |
| **架构** | Shortcut MoE | Thinker-Talker 双MoE | 多编码器融合 | 音频Tokenizer | 解耦视觉编码 |
| **上下文** | **128K** | 32K | 8K | - | - |
| **端到端延迟** | - | **234ms** (音频) | ~300ms | ~200ms | - |
| **模态支持** | 文/图/音/视 | 文/图/音/视 | 文/图/音/视 | 文/音 | 文/图 |
| **音频编码** | DFSMN | **AuT (自研)** | Whisper | GLM-4-Voice-Tokenizer | - |
| **位置编码** | RoPE | **TM-RoPE** | RoPE | RoPE | - |
| **训练数据** | - | 36T tokens | - | - | - |
| **开源协议** | 开源 | **Apache 2.0** | Apache 2.0 | 开源 | MIT |
| **最小显存** | 8×H20 141G | ~24GB | ~8GB | ~12GB | ~16GB |

### 音频处理能力对比

| 模型 | 帧粒度 | 编码器 | 最大音频长度 | 语音生成 |
|------|--------|--------|--------------|----------|
| LongCat-Flash-Omni | 80ms | DFSMN (22层) | - | 4-codebook |
| Qwen3-Omni | 80ms | **AuT (650M)** | **40分钟** | **MTP+Code2Wav** |
| MiniCPM-o 2.6 | 320ms | Whisper-medium | ~10分钟 | ChatTTS |
| GLM-4-Voice | 80ms | 自研Tokenizer | - | Flow Matching |

### 性能表现对比

| 模型 | ASR (WenetSpeech) | 视频理解 | 文本(MMLU) | 语音延迟 |
|------|-------------------|----------|------------|----------|
| LongCat-Flash-Omni | - | - | - | - |
| Qwen3-Omni | **4.69** (SOTA) | 70.5 (Video-MME) | 86.6 | **234ms** |
| MiniCPM-o 2.6 | ~6.0 | ~65 | ~75 | ~300ms |
| GPT-4o (参考) | 15.30 | 71.9 | 91.3 | ~200ms |

---

## LongCat-Flash-Omni 深度解析

> 美团 LongCat 团队开发的全能型 AI 模型 - 560B 参数 (27B 激活) 的多模态大模型

### 模型概述

| 项目 | 规格 |
|------|------|
| **总参数量** | 560B (5600 亿) |
| **激活参数** | 27B (~4.8%) |
| **架构** | Shortcut-connected MoE (Mixture-of-Experts) |
| **模态支持** | 文本、音频、图像、视频 |
| **上下文长度** | **128K tokens** |
| **音频帧粒度** | 80ms |
| **推理精度** | FP8 (单节点) / BF16 (多节点) |

### 整体架构图

```
┌─────────────────────────────────────────────────────────────────────┐
│                     LongCat-Flash-Omni 架构                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐         │
│    │  Visual      │    │   Audio      │    │    Text      │         │
│    │  Encoder     │    │   Encoder    │    │  Embedding   │         │
│    │  (Univitar)  │    │   (DFSMN)    │    │  (131K vocab)│         │
│    └──────┬───────┘    └──────┬───────┘    └──────┬───────┘         │
│           │                   │                   │                  │
│           └───────────────────┼───────────────────┘                  │
│                               ▼                                      │
│                    ┌──────────────────┐                               │
│                    │  Embedding Fusion│  ← 统一嵌入空间 (7168维)      │
│                    └────────┬─────────┘                               │
│                             │                                         │
│                             ▼                                         │
│                    ┌──────────────────┐                               │
│                    │   LongCat Flash  │  ← MoE 主干 (560B参数)        │
│                    │   (MoE Backbone) │     激活 27B                 │
│                    └────────┬─────────┘                               │
│                             │                                         │
│                             ▼                                         │
│                    ┌──────────────────┐                               │
│                    │  Output Heads     │                               │
│                    │  (Text + Audio)   │                               │
│                    └──────────────────┘                               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 视觉编码器 (Univitar)

**配置参数**:
```python
LongCatVisionConfig:
├── num_hidden_layers: 24          # Transformer 层数
├── num_attention_heads: 16        # 注意力头数
├── hidden_size: 1024              # 隐藏层维度
├── intermediate_size: 4224        # FFN 中间层维度
├── patch_size: 14                 # 空间 patch 大小
├── temporal_patch_size: 2         # 时间 patch 大小 (视频)
├── image_size: 1792               # 输入图像分辨率
└── attention_type: "flash_attention"
```

**处理流程**:
```
输入图像 (H×W×3)
    ↓
3D Convolution (kernel: [2, 14, 14])
    ↓
Patch Embeddings (1024维)
    ↓
24× Transformer Layers
    ├── FlashAttention (双向)
    ├── 2D Rotary Position Embedding
    ├── SwiGLU Activation
    └── RMSNorm
    ↓
Vision Projector (1024 → 7168)
    ↓
输出 (7168维, 与文本对齐)
```

### 音频编码器 (DFSMN)

**配置参数**:
```python
LongCatAudioConfig:
├── input_size: 1200      # fbank 特征维度
├── hidden_size: 6144     # FSMN 隐藏层
├── proj_size: 1536       # 投影层维度
├── nlayer: 22            # DFSMN 层数
├── ndnn: 2               # DNN 层数
├── left_order: 10        # 左记忆窗口 (800ms)
├── right_order: 1        # 右记忆窗口 (80ms)
└── activation: relu6
```

**DFSMN 架构**:
```
输入音频特征 (1200维 fbank)
    ↓
22× DFSMN Layers
    ├── Memory Block
    │   └── Depthwise 1D Conv (kernel_size = 12)
    └── FFN Block
        ├── LayerNorm
        ├── Linear (1200 → 6144)
        ├── ReLU6
        └── Linear (6144 → 1200)
    ↓
2× DNN Layers
    ↓
Audio Projector (1200 → 7168)
    ↓
输出 (7168维)
```

**音频帧粒度**: 每帧 **80ms**

### MoE 架构详解

```
┌─────────────────────────────────────────────────────────────────┐
│                    LongCat MoE 架构                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  输入Embedding (7168维)                                          │
│       ↓                                                         │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │                    MoE Layer                                ││
│  │  ┌─────────┐                                                ││
│  │  │  Router │  → 选择 Top-2 专家                             ││
│  │  └────┬────┘                                                ││
│  │       ↓                                                     ││
│  │  ┌────┴────┬────────┬────────┬────────┐                    ││
│  │  │ 专家0   │ 专家1  │ 专家2  │ ...   │ 专家63            ││
│  │  │ (文本)  │ (视觉) │ (音频) │       │ (混合)            ││
│  │  │ ~9B    │ ~9B    │ ~9B    │       │ ~9B               ││
│  │  └────┬────┴───┬────┴────────┘       └───────────────────┘││
│  │       └────┬───┘                                            ││
│  │            ↓                                                 ││
│  │      加权融合输出                                            ││
│  └─────────────────────────────────────────────────────────────┘│
│                                                                  │
│  每层激活参数: ~0.5B (2专家 × 每层参数)                           │
│  总激活参数: 27B                                                 │
│  总参数量: 560B                                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 推理流程

**完整调用链**:
```
longcat_omni_demo.py:main()
    │
    ├─→ init_global_config(args)
    │   └─→ set_global_variables(config)
    │
    ├─→ LoncatOmniInfer.__init__(args)
    │   │
    │   ├─→ build_modality_models()
    │   │   ├─→ TextEmbedding()
    │   │   ├─→ LongCatOmniVisionAdaptor()
    │   │   ├─→ LongCatOmniAudioAdaptor()
    │   │   ├─→ AudioEmbedding(audio_head_num=4)
    │   │   ├─→ DataProcessor()
    │   │   └─→ OmniUnifiedPostProcessor()
    │   │
    │   └─→ create_sglang_engine()
    │
    └─→ infer_engine.generate(input, sampling_params)
```

**单次推理详细流程**:
```python
# 步骤 1: 数据预处理
def _process_input(input_dict):
    data = self._input_processor.process(input_dict)
    # 返回: prompts, audios, audio_masks, images, grid_shapes

# 步骤 2: 嵌入生成
def _get_input_embedding(input_ids, codecs, audios, images):
    # 2.1 基础文本嵌入
    merged = self.text_embedding(input_ids)

    # 2.2 音频 codec 嵌入 (4个 codebook 相加)
    if codecs is not None:
        audio_embs = self.audio_embedding(codecs)
        for i in range(4):
            merged += audio_embs[i]

    # 2.3 连续音频嵌入 (替换 pad 位置)
    if audios is not None:
        audio_emb = self.audio_adaptor_model(audios, audio_masks)
        merged[audio_pad_mask] = audio_emb

    # 2.4 视觉嵌入 (替换 pad 位置)
    if images is not None:
        vision_emb = self.vision_adaptor_model(images, grid_shapes)
        merged[vision_pad_mask] = vision_emb

    return merged

# 步骤 3: SGLang 推理
async def generate():
    output = await self.sglang_engine.async_generate(
        input_embeds=input_embedding,
        sampling_params={"temperature": 1.0, "max_new_tokens": 4096}
    )
    return output

# 步骤 4: 后处理
def post_processor.process(output):
    text = tokenizer.decode(output["output_ids"])
    waveform = codec_decoder.decode(output["aux_info"]["audio_codes"])
    return ProcessedOutput(text=text, audio_waveform=waveform)
```

### Token 处理机制

**特殊 Token 定义**:
```python
# 音频相关
AUDIO_BOS_TOKEN = "<|audio|>"        # 音频开始
AUDIO_EOS_TOKEN = "<|/audio|>"       # 音频结束
AUDIO_PAD_TOKEN = "<|audio_pad|>"    # 音频填充占位符

# 视觉相关
IMAGE_PAD_TOKEN = "<|image_pad|>"    # 图像填充
DEFAULT_IMAGE_TOKEN = "<image>"

# 对话角色
SYSTEM_BOS_TOKEN = "<begin-of-system>"
USER_BOS_TOKEN = "<begin-of-user>"
ASSISTANT_BOS_TOKEN = "<begin-of-assistant>"

# 音频 Codec
CODEC_EOS_ID = 2
CODEC_PAD_ID = 3
NUM_CODEC_PLACEHOLDERS = 32
```

**嵌入融合策略**:
```python
# 文本: 直接 embedding
merged = text_embedding(input_ids)

# 音频 Codec: 相加融合
for i in range(4):
    merged += audio_embedding[i](codecs[:, :, i])

# 连续音频: 替换 pad 位置
merged[audio_pad_mask] = audio_adaptor_embedding

# 视觉: 替换 pad 位置
merged[vision_pad_mask] = vision_adaptor_embedding
```

**音频 Codec 结构**:
```
4-codebook 编码:
┌─────────────────────────────────────────────┐
│  Codebook 0: 语义 Token (Semantic)         │
│  Codebook 1-3: 声学 Token (Acoustic)       │
│                                             │
│  每帧 80ms → 4 个 token                    │
│  Codec ID 偏移: +32                         │
└─────────────────────────────────────────────┘
```

### 并行策略

**TP + EP 并行架构**:
```
┌────────────────────────────────────────────────────────────┐
│               TP + EP 并行架构                              │
├────────────────────────────────────────────────────────────┤
│                                                            │
│   Node 0                              Node 1              │
│   ┌────┬────┬────┬────┐              ┌────┬────┬────┬────┐│
│   │GPU0│GPU1│GPU2│GPU3│              │GPU4│GPU5│GPU6│GPU7││
│   ├────┼────┼────┼────┤              ├────┼────┼────┼────┤│
│   │ TP │ TP │ TP │ TP │              │ TP │ TP │ TP │ TP ││
│   │ EP │ EP │ EP │ EP │              │ EP │ EP │ EP │ EP ││
│   │ E0 │ E1 │ E2 │ E3 │              │ E4 │ E5 │ E6 │ E7 ││
│   └────┴────┴────┴────┘              └────┴────┴────┴────┘│
│                                                            │
│   TP (Tensor Parallelism): 模型层内切分                    │
│   EP (Expert Parallelism): MoE 专家分布                    │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**推荐配置**:

| 配置 | GPU | 精度 | 参数 |
|------|-----|------|------|
| 单节点 | 8× H20-141G | FP8 | `--tp-size 8 --ep-size 8` |
| 双节点 | 16× H800-80G | BF16 | `--tp-size 16 --ep-size 16 --nodes 2` |

### 硬件要求

**最低配置**:
- **GPU**: 单节点 8× H20-141G (141GB VRAM)
- **精度**: FP8
- **并行**: TP=8, EP=8

**推荐配置**:
- **GPU**: 双节点 16× H800-80G (80GB VRAM)
- **精度**: BF16
- **并行**: TP=16, EP=16

---

## Qwen3-Omni 深度解析

> 阿里巴巴 Qwen 团队 2025年9月发布的全模态端到端模型 - 首个无性能退化的多模态系统

### 模型概述

| 项目 | 规格 |
|------|------|
| **总参数量** | Thinker 30B + Talker 3B = **~34B** |
| **激活参数** | Thinker 3B + Talker 0.3B |
| **架构** | Thinker-Talker 双 MoE |
| **模态支持** | 文本、音频、图像、视频 |
| **上下文长度** | 32K tokens |
| **端到端延迟** | **234ms** (音频) / 547ms (视频) |
| **音频帧粒度** | 80ms |
| **训练数据** | 36T tokens |
| **开源协议** | Apache 2.0 |

### 核心成就

> **"全能不偏科"** - 36项音视频基准测试，**32项开源SOTA**

| 能力 | 表现 |
|------|------|
| ASR (WenetSpeech) | **4.69 WER** (超越 GPT-4o 的 15.30) |
| 视频理解 (Video-MME) | 70.5 |
| 文本 (MMLU) | 86.6 (与同规模单模态持平) |
| 多语言 | 119种文本 / 19种语音输入 / 10种语音输出 |
| 长音频 | 支持 **40分钟** 音频输入 |

### 整体架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Qwen3-Omni 架构全景                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                         感知层 (Perception)                            │  │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌───────────┐  │  │
│  │  │   文本输入    │  │   音频输入    │  │   图像输入    │  │  视频输入  │  │  │
│  │  │  Tokenizer   │  │   AuT编码器   │  │ SigLIP2编码器 │  │  (同上)   │  │  │
│  │  │  151K词汇表  │  │   650M参数   │  │   540M参数   │  │           │  │  │
│  │  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘  └─────┬─────┘  │  │
│  │         └─────────────────┴─────────────────┴────────────────┘        │  │
│  │                                    ↓                                  │  │
│  │                        TM-RoPE 位置编码                                │  │
│  │                    (时间对齐多模态旋转位置编码)                          │  │
│  └────────────────────────────────────┬───────────────────────────────────┘  │
│                                       ↓                                     │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                    Thinker (思考器) - MoE架构                          │  │
│  │                                                                       │  │
│  │   ┌─────────────────────────────────────────────────────────────┐     │  │
│  │   │              MoE Transformer (30B总参, 3B激活)               │     │  │
│  │   │   功能：多模态理解 + 文本生成                                │     │  │
│  │   │   输出：文本 Token + 高层语义特征 (给Talker)                  │     │  │
│  │   └─────────────────────────────────────────────────────────────┘     │  │
│  └────────────────────────────────────┬───────────────────────────────────┘  │
│                                       ↓                                     │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                    Talker (发声器) - MoE架构                           │  │
│  │                                                                       │  │
│  │   ┌─────────────────────────────────────────────────────────────┐     │  │
│  │   │              MoE Transformer (3B总参, 0.3B激活)              │     │  │
│  │   │   接收：Thinker的高层特征 + 历史对话上下文                    │     │  │
│  │   │   功能：流式语音生成                                          │     │  │
│  │   └────────────────────────┬────────────────────────────────────┘     │  │
│  │                            ↓                                          │  │
│  │   ┌─────────────────────────────────────────────────────────────┐     │  │
│  │   │                  MTP 模块 (80M参数)                          │     │  │
│  │   │         多Token预测 - 一次生成多个码本层                      │     │  │
│  │   └────────────────────────┬────────────────────────────────────┘     │  │
│  │                            ↓                                          │  │
│  │   ┌─────────────────────────────────────────────────────────────┐     │  │
│  │   │                Code2Wav (200M参数)                           │     │  │
│  │   │         轻量级因果卷积网络 - 波形合成                          │     │  │
│  │   └────────────────────────┬────────────────────────────────────┘     │  │
│  │                            ↓                                          │  │
│  │                      音频波形输出                                       │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 五大核心升级（相比 Qwen2.5-Omni）

```
┌─────────────────────────────────────────────────────────────────────┐
│              Qwen3-Omni vs Qwen2.5-Omni 升级对比                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. 双MoE设计                                                        │
│     Qwen2.5: Dense Thinker + Dense Talker                           │
│     Qwen3:   MoE Thinker (30B-A3B) + MoE Talker (3B-A0.3B)          │
│              └── 高并发下延迟更稳定，TPS更高                          │
│                                                                      │
│  2. 音频编码器升级                                                    │
│     Qwen2.5: Whisper (外部预训练)                                    │
│     Qwen3:   AuT (自研, 2000万小时训练)                              │
│              └── 中文ASR更强，支持实时预填充缓存                       │
│                                                                      │
│  3. 语音表示升级                                                      │
│     Qwen2.5: 单码本                                                  │
│     Qwen3:   多码本 (8层) + MTP多Token预测                           │
│              └── 更精细的音色和副语言建模                             │
│                                                                      │
│  4. 延迟优化                                                          │
│     Qwen2.5: 扩散模型声码器 (DiT)                                    │
│     Qwen3:   Code2Wav轻量级卷积网络 (200M)                           │
│              └── 首包延迟 234ms                                      │
│                                                                      │
│  5. 位置编码升级                                                      │
│     Qwen2.5: M-RoPE (固定2秒分块)                                    │
│     Qwen3:   TM-RoPE (时间锚定，灵活对齐)                            │
│              └── 支持任意时长流式输入                                 │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```


### AuT 音频编码器（自研）

**为什么替代 Whisper？**

```
┌─────────────────────────────────────────────┐
│              AuT (Audio Transformer)         │
│                 650M 参数                     │
├─────────────────────────────────────────────┤
│                                              │
│  输入: 原始音频波形 (16kHz)                   │
│       ↓                                      │
│  ┌─────────────────────┐                    │
│  │   Conv2D 下采样      │  ← 8倍下采样        │
│  │   (滤波器组特征)      │                    │
│  └─────────────────────┘                    │
│       ↓                                      │
│  ┌─────────────────────┐                    │
│  │  动态窗口 FlashAttention                 │
│  │  窗口: 1-8秒可调                         │
│  │  Token率: 12.5 Hz (每80ms一帧)           │
│  └─────────────────────┘                    │
│       ↓                                      │
│  输出: 音频特征向量                           │
│                                              │
│  训练数据: 2000万小时有监督音频               │
│  - 80% 中英文ASR                             │
│  - 10% 其他语言ASR                           │
│  - 10% 音频理解数据                           │
│                                              │
└─────────────────────────────────────────────┘
```

**性能对比**:

| 数据集 | GPT-4o | Gemini-2.5 | Qwen2.5-Omni | **Qwen3-Omni** |
|--------|--------|------------|--------------|----------------|
| Librispeech | 1.39 | 2.89 | 1.74 | **1.22** |
| Wenetspeech | 15.30 | 14.43 | 5.91 | **4.69** |

### TM-RoPE：时间对齐多模态位置编码

**核心创新**：音视频精确同步，支持长序列

```
┌─────────────────────────────────────────────────────────────┐
│              TM-RoPE (Time-aligned M-RoPE)                  │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  维度分配 (相比M-RoPE优化):                                   │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  - 时间维度: 24个旋转角度 (M-RoPE: 16)                 │  │
│  │  - 高度维度: 20个旋转角度                              │  │
│  │  - 宽度维度: 20个旋转角度                              │  │
│  │                                                       │  │
│  │  更多时间维度 → 更好的长序列外推能力                    │  │
│  └───────────────────────────────────────────────────────┘  │
│                                                              │
│  不同模态的处理:                                              │
│                                                              │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐                 │
│  │  文本    │    │  音频    │    │  图像/视频│                 │
│  │         │    │         │    │         │                 │
│  │ T/H/W   │    │ T/H/W   │    │ T/H/W   │                 │
│  │ 相同ID  │    │ 相同ID  │    │ T:帧ID  │                 │
│  │         │    │ +绝对时间│    │ H/W:位置│                 │
│  │         │    │ 80ms/步 │    │         │                 │
│  └─────────┘    └─────────┘    └─────────┘                 │
│                                                              │
│  音视频同步机制:                                              │
│  - 音频: 每80ms一个时间ID                                     │
│  - 视频: 按实际时间戳对齐到80ms粒度                            │
│  - 连续编号: 后一模态起始位置 = 前一模态最大位置 + 1            │
│                                                              │
│  优势: 支持任意时长流式输入，无需固定分块                       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 流式语音生成机制

**MTP + Code2Wav 详解**:

```
┌──────────────────────────────────────────────────────────────────────┐
│                    流式语音生成流程                                   │
├──────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  传统方式 (逐个预测)              Qwen3方式 (MTP一次预测)              │
│                                                                       │
│  Step1: [C0] ────────→          Step1: [C0,C1,C2,C3,C4,C5,C6,C7]    │
│  Step2: [C1] ────────→                   ↑                          │
│  Step3: [C2] ────────→          MTP模块: 一次预测所有残差码本         │
│  ...                                                              │
│  Step8: [C7] ────────→                                              │
│                                                                       │
│  延迟: 8步 × 每步时间              延迟: 1步 × 每步时间                │
│                                                                       │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  │
│                                                                       │
│  Code2Wav 波形合成:                                                    │
│                                                                       │
│  Talker输出 ──→ MTP预测残差码本 ──→ Code2Wav(因果ConvNet) ──→ 波形    │
│                                                                       │
│  对比:                                                                │
│  ┌─────────────────┐    ┌─────────────────┐                          │
│  │   扩散模型 DiT   │    │  轻量ConvNet    │                          │
│  │   (计算密集型)   │ →  │  (200M参数)     │                          │
│  │   需等上下文    │    │  单帧即可合成   │                          │
│  │   延迟高        │    │  延迟极低       │                          │
│  └─────────────────┘    └─────────────────┘                          │
│                                                                       │
│  首包延迟: 234ms (音频) / 547ms (视频)                                │
│                                                                       │
└──────────────────────────────────────────────────────────────────────┘
```

### 分块预填充机制

```
┌─────────────────────────────────────────────────────────────────────┐
│                    分块预填充流程                                    │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  用户输入音频/视频流                                                  │
│       ↓                                                             │
│  ┌─────────────────────────────────────────────────────────────────┐│
│  │  Block 0 (2秒)  ─────────────────────────────────────────────→  ││
│  │       ↓ (Thinker处理)                                            ││
│  │       ↓ (同时)                                                  ││
│  │  Block 1 (2秒)  ─────────────────────────────────────────────→  ││
│  │       ↓ (Thinker处理) ──→ 输出给Talker ──→ 开始生成语音        ││
│  │       ↓ (同时)                                                  ││
│  │  Block 2 (2秒)  ─────────────────────────────────────────────→  ││
│  │       ↓ (Thinker处理下一块...)                                    ││
│  │                                                                  ││
│  │  关键: Thinker和Talker并行处理！                                  ││
│  │       - Thinker处理当前块时，Talker在生成上一块的语音             ││
│  │       - 大幅降低首Token时间 (TTFT)                               ││
│  └─────────────────────────────────────────────────────────────────┘│
│                                                                      │
│  并发优化:                                                            │
│  - MoE架构减少KV Cache IO消耗                                        │
│  - 轻量级MTP和Code2Wav支持批量推理                                    │
│  - 不同并发下延迟稳定                                                  │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 三阶段训练策略

```
┌─────────────────────────────────────────────────────────────────────┐
│                      三阶段预训练                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Stage 1: 编码器对齐 (S1)                                             │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  - 冻结LLM参数                                                  │ │
│  │  - 只训练视觉编码器(SigLIP)和音频编码器(AuT)                     │ │
│  │  - 目标: 让编码器输出与LLM输入空间对齐                            │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                               ↓                                      │
│  Stage 2: 通用训练 (S2)                                               │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  - 训练所有参数                                                 │ │
│  │  - 数据量: 2万亿 tokens                                         │ │
│  │    ├─ 文本:   0.57T (28.5%)                                    │ │
│  │    ├─ 音频:   0.77T (38.5%)                                    │ │
│  │    ├─ 图像:   0.82T (41%)                                      │ │
│  │    ├─ 视频:   0.05T (2.5%)                                     │ │
│  │    └─ 音视频: 0.05T (2.5%)                                     │ │
│  │  - 关键: 早期就融合单模态和跨模态数据！                           │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                               ↓                                      │
│  Stage 3: 长上下文 (S3)                                               │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  - 最大长度扩展到 32,768 tokens                                 │ │
│  │  - 增加长音频(40分钟+)和长视频比例                                │ │
│  │  - 提升长程依赖建模能力                                          │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 核心架构对比分析

### 1. 整体架构范式对比

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        两种架构范式对比                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  LongCat-Flash-Omni (单流MoE)          Qwen3-Omni (双流MoE)                  │
│                                                                              │
│  ┌─────────────────────────┐          ┌─────────────────────────┐           │
│  │      多模态输入          │          │      多模态输入          │           │
│  │  文/图/音/视 → Embedding │          │  文/图/音/视 → Embedding │           │
│  └──────────┬──────────────┘          └──────────┬──────────────┘           │
│             ↓                                     ↓                          │
│  ┌─────────────────────────┐          ┌─────────────────────────┐           │
│  │    Unified MoE Backbone  │          │    Thinker (思考器)      │           │
│  │    (560B / 激活27B)      │          │    MoE 30B-A3B          │           │
│  │                         │          │    功能:理解+文本生成     │           │
│  │    统一处理所有任务      │          └──────────┬──────────────┘           │
│  │    (理解+生成)           │                     ↓                          │
│  └──────────┬──────────────┘          ┌─────────────────────────┐           │
│             ↓                         │    Talker (发声器)       │           │
│  ┌─────────────────────────┐          │    MoE 3B-A0.3B         │           │
│  │    Output Heads          │          │    功能:语音生成         │           │
│  │    文本 + 音频           │          └──────────┬──────────────┘           │
│  └─────────────────────────┘                     ↓                          │
│                                       ┌─────────────────────────┐           │
│                                       │    MTP + Code2Wav        │           │
│                                       │    流式波形生成          │           │
│                                       └─────────────────────────┘           │
│                                                                              │
│  特点:                                特点:                                  │
│  - 架构简单，统一优化                  - 解耦设计，可独立控制                 │
│  - 适合大规模部署                      - 适合流式实时交互                     │
│  - 上下文更长(128K)                    - 延迟更低(234ms)                      │
│  - 参数规模更大                        - 支持外部干预(如RAG)                  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2. 音频处理架构对比

| 维度 | LongCat-Flash-Omni | Qwen3-Omni |
|------|-------------------|------------|
| **编码器** | DFSMN (22层) | **AuT (自研)** |
| **参数量** | ~300M | **650M** |
| **注意力机制** | 1D Conv 记忆块 | **FlashAttention + 动态窗口** |
| **Token率** | 12.5 Hz | 12.5 Hz |
| **帧粒度** | 80ms | 80ms |
| **训练数据** | - | **2000万小时** |
| **中文优化** | 一般 | **强** |
| **实时缓存** | 不支持 | **支持** |

**DFSMN vs AuT 架构对比**:

```
DFSMN (LongCat)                    AuT (Qwen3)
┌─────────────────┐               ┌─────────────────┐
│  1D Conv 记忆块  │               │  Conv2D 下采样   │
│  (固定窗口)      │               │  (8倍)           │
└────────┬────────┘               └────────┬────────┘
         ↓                                 ↓
┌─────────────────┐               ┌─────────────────┐
│  FFN Block      │               │  Transformer    │
│  (ReLU6)        │               │  (动态窗口注意力)│
└────────┬────────┘               └────────┬────────┘
         ↓                                 ↓
    22层堆叠                          多层堆叠
         ↓                                 ↓
    投影到LLM空间                    投影到LLM空间

特点:                            特点:
- 计算高效                        - 表达能力更强
- 适合固定长度音频                - 支持变长/流式
- 传统语音识别                    - 通用音频理解
```

### 3. 语音生成架构对比

| 维度 | LongCat-Flash-Omni | Qwen3-Omni |
|------|-------------------|------------|
| **表示方式** | 4-codebook | **8-codebook (多码本)** |
| **码本预测** | 逐帧预测 | **MTP多Token预测** |
| **波形合成** | Codec Decoder | **Code2Wav (因果ConvNet)** |
| **合成延迟** | 高 | **极低** |
| **首包延迟** | - | **234ms** |
| **音色控制** | 较好 | **精细** |
| **流式支持** | 支持 | **支持 (更优)** |

**生成流程对比**:

```
LongCat-Flash-Omni:              Qwen3-Omni:

[Codebook 0] ──┐                [Codebook 0] ──→ MTP ──┬──→ [C1,C2,C3...C7]
[Codebook 1] ──┤──→ Codec      [Codebook 0] (下一帧) ──┤
[Codebook 2] ──┤    Decoder    ...                      │
[Codebook 3] ──┘                         ↓               │
                              Code2Wav (因果ConvNet)      │
                                         ↓               │
                                    波形输出              │

延迟: 高 (需等完整frame)        延迟: 极低 (单帧即可合成)
```

### 4. 位置编码对比

| 维度 | LongCat-Flash-Omni | Qwen3-Omni |
|------|-------------------|------------|
| **类型** | RoPE (标准) | **TM-RoPE (时间对齐)** |
| **维度** | 1D | **3D (时间/高度/宽度)** |
| **时间编码** | 相对位置 | **绝对时间锚定** |
| **音视频同步** | 固定分块对齐 | **灵活时间戳对齐** |
| **长序列** | 128K | 32K |
| **流式支持** | 支持 | **支持 (更灵活)** |

### 5. MoE 架构对比

```
LongCat-Flash-Omni:              Qwen3-Omni:

单MoE (大而全)                   双MoE (小而专)
┌─────────────────┐              ┌─────────────────┐
│  560B 总参数     │              │ Thinker: 30B    │
│  27B 激活       │              │   (激活3B)      │
│  64+ 专家       │              │ Talker: 3B      │
│                 │              │   (激活0.3B)    │
│  所有任务共享    │              │                 │
│  同一套专家      │              │  专家分工明确   │
└─────────────────┘              └─────────────────┘

优势:                           优势:
- 参数规模大，容量大             - 延迟可控
- 统一优化                      - 高并发稳定
- 适合批处理                    - 适合流式服务
```

### 6. 训练策略对比

| 维度 | LongCat-Flash-Omni | Qwen3-Omni |
|------|-------------------|------------|
| **训练阶段** | 未公开 | **三阶段 (S1/S2/S3)** |
| **数据规模** | - | **36T tokens** |
| **多模态融合** | 早期融合 | **早期融合+渐进式** |
| **长上下文训练** | 支持 | **专门阶段 (S3)** |
| **开源程度** | 模型开源 | **完全开源 (含技术细节)** |

---

## 技术演进趋势

### 开源 Omni 模型发展趋势

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    开源 Omni 模型演进时间线                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  2024                                    2025                                │
│    │                                      │                                  │
│    ├── 2024.05: Mini-Omni (首个开源)      ├── 2025.01: MiniCPM-o 2.6        │
│    │           端到端语音                  │           端侧全模态            │
│    │                                      │                                  │
│    ├── 2024.10: GLM-4-Voice               ├── 2025.03: Qwen2.5-Omni        │
│    │           情感语音                    │           Thinker-Talker        │
│    │                                      │                                  │
│    └── 2024: LongCat-Flash-Omni           ├── 2025.09: Qwen3-Omni          │
│                大规模MoE                  │           双MoE+无性能退化       │
│                                           │                                  │
│                                           └── 2025.01: Janus-Pro           │
│                                                       理解生成统一           │
│                                                                              │
│  趋势1: 架构从 Dense → MoE → 双MoE (专业化)                                  │
│  趋势2: 延迟从高 → 低 → 极低 (234ms)                                         │
│  趋势3: 部署从云端 → 端侧 → 端云协同                                         │
│  趋势4: 开源从模型 → 模型+数据+训练细节                                       │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 关键技术创新图谱

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       技术创新关联图谱                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌──────────────┐                                                           │
│  │  基础架构     │                                                           │
│  │  Transformer │                                                           │
│  └──────┬───────┘                                                           │
│         ↓                                                                   │
│  ┌──────────────┬──────────────┬──────────────┐                            │
│  │    MoE       │   多模态融合  │   流式生成    │                            │
│  │              │              │              │                            │
│  │  ┌────────┐  │  ┌────────┐  │  ┌────────┐  │                            │
│  │  │LongCat │  │  │ TM-RoPE│  │  │  MTP   │  │                            │
│  │  │ 560B   │  │  │ Qwen3  │  │  │ Qwen3  │  │                            │
│  │  └────────┘  │  └────────┘  │  └────────┘  │                            │
│  │  ┌────────┐  │  ┌────────┐  │  ┌────────┐  │                            │
│  │  │Qwen3   │  │  │早期融合│  │  │Code2Wav│  │                            │
│  │  │双MoE   │  │  │LongCat │  │  │ Qwen3  │  │                            │
│  │  └────────┘  │  └────────┘  │  └────────┘  │                            │
│  └──────────────┴──────────────┴──────────────┘                            │
│                                                                              │
│  发展方向:                                                                  │
│  1. MoE → 更细粒度的专家分工 (Thinker/Talker分离)                           │
│  2. 多模态融合 → 更精确的时间/空间对齐 (TM-RoPE)                            │
│  3. 流式生成 → 更低延迟 (MTP+轻量级声码器)                                  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 选型建议

### 按场景选择

| 应用场景 | 推荐模型 | 理由 |
|----------|----------|------|
| **企业级大规模服务** | LongCat-Flash-Omni | 560B参数，128K上下文，适合批处理 |
| **实时语音助手** | Qwen3-Omni | 234ms延迟，双MoE架构，流式优化 |
| **端侧/手机部署** | MiniCPM-o 2.6 | 8B参数，全双工，消费级GPU可跑 |
| **多语言场景** | Qwen3-Omni | 119种文本语言，中文ASR最强 |
| **情感陪伴/娱乐** | GLM-4-Voice | 情感控制，方言支持 |
| **图像生成+理解** | Janus-Pro | 统一架构，文生图+图理解 |
| **嵌入式/IoT** | Mini-Omni2 | 1.2GB体积，极致轻量 |
| **研究/学习** | Qwen3-Omni | 完全开源，技术报告详细 |

### 按技术栈选择

| 技术需求 | 推荐模型 | 关键特性 |
|----------|----------|----------|
| **MoE架构研究** | LongCat / Qwen3 | 大规模MoE实践 |
| **流式生成优化** | Qwen3 | MTP + Code2Wav |
| **音频编码器设计** | Qwen3 | AuT自研编码器 |
| **位置编码创新** | Qwen3 | TM-RoPE时间对齐 |
| **端侧优化** | MiniCPM-o | 全双工低功耗 |
| **多模态对齐** | LongCat | 早期融合策略 |

### 部署成本对比

```
┌─────────────────────────────────────────────────────────────────┐
│                     部署成本估算                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  LongCat-Flash-Omni:                                            │
│  ├─ 硬件: 8× H20-141G (单节点)                                   │
│  ├─ 成本: ~$20万 (硬件)                                          │
│  ├─ 功耗: ~4000W                                                 │
│  └─ 适合: 大企业/云服务                                          │
│                                                                  │
│  Qwen3-Omni:                                                    │
│  ├─ 硬件: 2× A100 80G 或 4× RTX 4090                             │
│  ├─ 成本: ~$1-3万 (硬件)                                         │
│  ├─ 功耗: ~1000W                                                 │
│  └─ 适合: 中型企业/研究机构                                      │
│                                                                  │
│  MiniCPM-o 2.6:                                                 │
│  ├─ 硬件: 1× RTX 4090 或 苹果M系列                               │
│  ├─ 成本: ~$2000 (硬件)                                          │
│  ├─ 功耗: ~300W                                                  │
│  └─ 适合: 小团队/端侧应用                                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 关键文件索引

### LongCat-Flash-Omni

| 功能模块 | 文件路径 |
|---------|---------|
| 推理入口 | `longcat_omni_demo.py` |
| 视觉编码器 | `encoders/vision_adaptor.py` |
| 音频编码器 | `encoders/audio_adaptor.py` |
| 文本嵌入 | `encoders/embedding.py` |
| 数据处理 | `data/data_processor.py` |
| 多模态分词器 | `data/multimodal_tokenizer.py` |
| 后处理 | `post_process/unified_post_processor.py` |
| 常量定义 | `constants.py` |
| 全局配置 | `global_vars.py` |

### Qwen3-Omni

| 功能模块 | 文件路径 | 说明 |
|---------|---------|------|
| 推理入口 | `qwen3_omni_demo.py` | 主入口 |
| Thinker | `modeling_thinker.py` | MoE思考器 |
| Talker | `modeling_talker.py` | MoE发声器 |
| AuT编码器 | `modeling_aut.py` | 自研音频编码器 |
| MTP模块 | `modeling_mtp.py` | 多Token预测 |
| Code2Wav | `modeling_code2wav.py` | 轻量级声码器 |
| TM-RoPE | `modeling_rope.py` | 时间对齐位置编码 |

---

## 相关链接

### LongCat-Flash-Omni
- [GitHub Repository](https://github.com/meituan-longcat/LongCat-Flash-Omni)
- [技术报告](待补充)

### Qwen3-Omni
- [GitHub Repository](https://github.com/QwenLM/Qwen3-Omni)
- [技术报告](https://arxiv.org/abs/2509.17765)
- [Hugging Face](https://huggingface.co/Qwen)

### 其他模型
- [MiniCPM-o 2.6](https://github.com/OpenBMB/MiniCPM-o)
- [GLM-4-Voice](https://github.com/THUDM/GLM-4-Voice)
- [Janus-Pro](https://github.com/deepseek-ai/Janus)
- [SGLang](https://github.com/sgl-project/sglang)

---

## 许可证

本分析文档基于以下开源项目编写：
- LongCat-Flash-Omni (开源)
- Qwen3-Omni (Apache 2.0)
- MiniCPM-o (Apache 2.0)
- GLM-4-Voice (开源)
- Janus-Pro (MIT)

---

*最后更新: 2025年*

*贡献: 欢迎提交 PR 补充更多模型分析*
